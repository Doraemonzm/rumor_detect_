{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "train_file=os.path.join('AAAI_dataset', 'politi_train.csv')\n",
    "test_file=os.path.join('AAAI_dataset', 'politi_test.csv')\n",
    "def read_csv(file):\n",
    "    dataset=[]\n",
    "    with open('{}'.format(file), encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        for idx,row in enumerate(reader):\n",
    "            if idx >= 1:\n",
    "                text = row[1]\n",
    "                img_name=row[2]\n",
    "                label=row[3]\n",
    "                if label == '0':\n",
    "                    dataset.append((text, img_name, 0))\n",
    "                elif label=='1':\n",
    "                    dataset.append((text, img_name, 1))\n",
    "    return dataset\n",
    "train_dataset=read_csv(train_file)\n",
    "test_dataset=read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(381, 104)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset),len(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 10, 14, 24, 27, 30, 33, 36, 38, 39, 48, 49, 62, 63, 65, 70, 72, 75, 76, 77, 78, 79, 81, 83, 84, 86, 91, 93, 96, 101, 108, 109, 110, 114, 116, 117, 118, 122, 123, 128, 131, 132, 135, 137, 138, 139, 141, 142, 143, 147, 151, 154, 156, 157, 165, 166, 168, 169, 176, 178, 179, 185, 186, 187, 188, 189, 190, 193, 194, 196, 197, 200, 201, 203, 204, 206, 211, 212, 215, 220, 223, 225, 226, 228, 231, 238, 241, 242, 250, 251, 253, 256, 258, 261, 268, 269, 271, 277, 278, 279, 286, 287, 292, 293, 298, 301, 303, 304, 305, 312, 318, 319, 323, 324, 325, 337, 338, 341, 343, 344, 345, 346, 351, 353, 356, 361, 363, 364, 367, 369, 370, 372, 373, 377], [2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 28, 29, 31, 32, 34, 35, 37, 40, 41, 42, 43, 44, 45, 46, 47, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 66, 67, 68, 69, 71, 73, 74, 80, 82, 85, 87, 88, 89, 90, 92, 94, 95, 97, 98, 99, 100, 102, 103, 104, 105, 106, 107, 111, 112, 113, 115, 119, 120, 121, 124, 125, 126, 127, 129, 130, 133, 134, 136, 140, 144, 145, 146, 148, 149, 150, 152, 153, 155, 158, 159, 160, 161, 162, 163, 164, 167, 170, 171, 172, 173, 174, 175, 177, 180, 181, 182, 183, 184, 191, 192, 195, 198, 199, 202, 205, 207, 208, 209, 210, 213, 214, 216, 217, 218, 219, 221, 222, 224, 227, 229, 230, 232, 233, 234, 235, 236, 237, 239, 240, 243, 244, 245, 246, 247, 248, 249, 252, 254, 255, 257, 259, 260, 262, 263, 264, 265, 266, 267, 270, 272, 273, 274, 275, 276, 280, 281, 282, 283, 284, 285, 288, 289, 290, 291, 294, 295, 296, 297, 299, 300, 302, 306, 307, 308, 309, 310, 311, 313, 314, 315, 316, 317, 320, 321, 322, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 339, 340, 342, 347, 348, 349, 350, 352, 354, 355, 357, 358, 359, 360, 362, 365, 366, 368, 371, 374, 375, 376, 378, 379, 380]]\n",
      "135\n",
      "246\n"
     ]
    }
   ],
   "source": [
    "label_set=[[],[]]\n",
    "for idx, data in enumerate(train_dataset):\n",
    "    _,_,label=data\n",
    "    if label==0:\n",
    "        label_set[0].append(idx)\n",
    "    else:\n",
    "        label_set[1].append(idx)\n",
    "print(label_set)\n",
    "print(len(label_set[0]))\n",
    "print(len(label_set[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Democrats go full TYRANNY: Now demand nationwide gun confiscation from law-abiding Americans… at gunpoint, of course by: Ethan Huff\\n\\nNaturalNews.com\\n\\nMonday, May 07, 2018\\n\\nA California Democrat was recently given a platform by USA Today to publish a shocking editorial that calls for nationwide confiscation of all “military-style semiautomatic assault weapons” from law-abiding citizens.\\n\\nRepresentative Eric Swalwell from California’s 15th District wrote that all so-called “assault” weapons need to be banned, and that a federal gun buy-back program needs to be instituted in order to effectively collect them all from the citizenry.\\n\\nRep. Swalwell even goes a step further, insisting that those who refuse to hand over their “assault” weapons be criminally prosecuted – including law-abiding gun owners who have never been convicted of committing a crime with their legally-purchased weaponry.\\n\\nNot content to simply impose a fresh ban on all new “assault” weapon purchases, which in and of itself is unconstitutional, Rep. Swalwell actually wants to see door-to-door gun confiscation teams engage in Nazi-style removal tactics in order to rid the streets of all firearms that he personally deems reckless and unnecessary.\\n\\n“Reinstating the federal assault weapons ban that was in effect from 1994 to 2004 would prohibit manufacture and sales, but it would not affect weapons already possessed,” Rep. Swalwell wrote in his op-ed for USA Today. “This would leave millions of assault weapons in our communities for decades to come.”\\n\\n“Instead, we should ban possession of military-style semiautomatic assault weapons, we should buy back such weapons from all who choose to abide by the law, and we should criminally prosecute any who choose to defy it by keeping their weapons.”\\n\\nDemocrats like Rep. Swalwell are enemies of the Constitution, and enemies of We the People\\n\\nThe Rest…HERE',\n",
       " '0fAqwAY4nXClIPGFaiu3Awb9uFuHUtLi.jpg',\n",
       " 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Philando Castile Had Been Stopped 52 Times By Police – WCCO \\n\\n\\n\\n— When Philando Castile saw the flashing lights in his rearview mirror the night he got shot, it wasn’t unusual. He had been pulled over at least 52 times in recent years in and around the Twin Cities and given citations for minor offenses including speeding, driving without a muffler and not wearing a seat belt.\\n\\nHe was assessed at least $6,588 in fines and fees, although more than half of the total 86 violations were dismissed, court records show.\\n\\nWas Castile an especially bad driver or just unlucky? Or was he targeted by officers who single out black motorists like him for such stops, as several of his family members have alleged?\\n\\nThe answer may never be known, but Castile’s stop for a broken tail light Wednesday ended with him fatally shot by a suburban St. Paul police officer, and Castile’s girlfriend livestreaming the chilling aftermath.\\n\\nThe shooting has added a new impetus to a national debate on racial profiling; a day after Castile died, a black Army veteran killed five officers in Dallas at a demonstration over Castile’s killing and another fatal police shooting, in Louisiana.\\n\\nThe Castile video “is pretty horrific,” said Gavin Kearney, who in 2003 co-authored a report to the Minnesota Legislature on racial profiling in the state. “There are things we don’t know about it. But we know there are certain assumptions and biases — whether explicit or implicit — about black men that affect how police officers interpret their actions. And we know white drivers are less likely to be pulled over.”\\n\\nCourt records dating to 2002 show Castile, a 32-year-old school cafeteria supervisor, averaged more than three traffic stops per year and received citations for misdemeanors or petty misdemeanors.\\n\\nMany charges were dismissed, but Castile pleaded guilty to some, mostly for driving after his license was revoked and driving with no proof of insurance. However, those two charges also were the most frequently dismissed, along with failing to wear a seat belt.\\n\\nThe records show no convictions for more serious crimes.\\n\\nNo recent information is available on the racial breakdown of drivers stopped or ticketed by police in Falcon Heights, the mostly white suburb where the shooting occurred, or in other Minnesota towns. Minnesota is not among the handful of states that require police to keep such data.\\n\\nBut in 2001, the Legislature asked for a racial profiling study and it fell to Kearney, then at the Institute on Race & Poverty at the University of Minnesota Law School, to conduct it. His study, using information supplied voluntarily by 65 law enforcement jurisdictions in the state, found a strong likelihood that racial and ethnic bias played a role in traffic stop policies and practices. Overall, officers stopped minority drivers at greater rates than whites and searched them at greater rates, but found contraband in those searches at lower rates than whites.\\n\\nThe analysis found the pattern was more pronounced in suburban areas. In Fridley, New Hope, Plymouth, Sauk Rapids and Savage combined, blacks were stopped about 310 percent more often than expected.\\n\\nThe St. Anthony Police Department, which employs the officer who shot Castile, did not participate in the study. St. Anthony officials have not commented on Castile’s stop since shortly after the shooting.\\n\\nIt was not immediately clear how much money governments in the Minneapolis-St. Paul area generate from traffic violations. A U.S. Department of Justice investigation following the 2014 police shooting death of Michael Brown, a black, unarmed 18-year-old, in Ferguson, Missouri, found law enforcement efforts were focused on generating revenue for that city. Most of the tickets and fines were going to blacks.\\n\\nCastile’s girlfriend, Diamond Reynolds, a passenger in the car, said the two officers who stopped them said the vehicle had a broken tail light. She said one of the officers shot him “for no apparent reason” after he reached for his ID.\\n\\nValerie Castile said she thinks her son “was just black in the wrong place.” Minnesota Gov. Mark Dayton said he did not believe it would have happened to a white motorist.\\n\\nThe officer who shot Castile, Jeronimo Yanez, is Latino. His lawyer, Thomas Kelly, said Saturday that his client reacted to the fact that Castile had a gun, not his race, though Kelly would not discuss what led Yanez to initiate the traffic stop.\\n\\n“Police understand the concerns about choices made about who gets stopped and what happens when they get stopped,” said Darrel Stephens, executive director of the Major Cities Chiefs Association.\\n\\nBut the statistics can’t simply be attributed to racial bias among police.\\n\\n“When people call the police, they provide a description of somebody engaged in a crime. The police respond to those descriptions,” said Stephens, a former Charlotte, North Carolina, police chief. “That counts for part of the disproportionality that we see in those numbers.”\\n\\nLast year, the President’s Task Force on 21st Century Policing recommended police departments collect and analyze demographic data on all stops, searches and seizures.\\n\\nNationally, 13 percent of black drivers were pulled over at least once in 2011, compared with 10 percent of the white drivers, according to a survey by the U.S. Justice Department’s Bureau of Justice Statistics.\\n\\nThe survey shows 68 percent of black drivers considered the stops legitimate compared with 84 percent of white drivers.\\n\\nThe precise reasons why certain motorists are pulled over more than others are difficult to identify, said Lorie Fridell, an associate professor of criminology at the University of South Florida, who trains police departments through a program called Fair and Impartial Policing.\\n\\n“Our implicit biases are most likely to impact us when we’re facing ambiguous situations,” Fridell said. “A person reaching into a pocket is ambiguous. If I, as a white, middle-aged woman, reach into my pocket most people aren’t going to experience fear. For a black male with dreadlocks, that ambiguous action would produce fear in many people.”\\n\\n(© Copyright 2016 The Associated Press. All Rights Reserved. This material may not be published, broadcast, rewritten or redistributed.)\\n\\nRelated Stories\\n\\nLawyer: Officer Who Shot Castile Reacted To Gun, Not Race\\n\\nOfficer Who Shot Philando Castile ‘Incredibly Sad’ For His Family\\n\\nRev. Jesse Jackson Meets With Dayton, Castile’s Girlfriend In St. Paul\\n\\nCo. Attorney Undecided On Using Grand Jury In Falcon Heights Shooting\\n\\nFalcon Heights Officials Meet With Civil Rights Leaders, Protest Planned\\n\\nDayton Urges Nonviolence, Patience During Shooting Investigation\\n\\nRep. Zerwas: Dayton ‘Made Things Worse’ With Comments On OIS\\n\\nCo. Prosecutor Has Not Met With Castile Family\\n\\nShooting Victim’s Girlfriend Speaks Out On Dallas\\n\\nBLM Supporters Condemn Violence During Dallas Protest\\n\\nJay-Z Releases Protest Song Following Police Shootings\\n\\nNeighbors Support Demonstrations At Governor’s Mansion\\n\\nPolice Fatally Shoot Man During Traffic Stop, Aftermath Video Posted',\n",
       " 'AjwE8TpO3NRa02C5d7WmKoe75QSuL6sT.jpg',\n",
       " 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import XLNetTokenizer, XLNetModel\n",
    "from conf import model_config_bert as model_config\n",
    "class LinearEmbed(nn.Module):\n",
    "    \"\"\"Embedding module\"\"\"\n",
    "    def __init__(self, dim_in=128, dim_out=2):\n",
    "        super(LinearEmbed, self).__init__()\n",
    "        self.linear = nn.Linear(dim_in, dim_out)\n",
    "        # self.l2norm = Normalize(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.linear(x)\n",
    "        # x = self.l2norm(x)\n",
    "        return x\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"data/pretrain_model/twitter/xlnet-base-cased\")\n",
    "model_text = XLNetModel.from_pretrained(\"data/pretrain_model/twitter/xlnet-base-cased\")\n",
    "text_classifer=LinearEmbed(15360,2)\n",
    "model_text.to('cuda')\n",
    "text_classifer.to('cuda')\n",
    "def text_embedding(text, text_model_path=None):\n",
    "    emb_set=[]\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    if text_model_path:\n",
    "        \n",
    "        checkpoint_text = torch.load(text_model_path)\n",
    "        model_text.load_state_dict(checkpoint_text['model'])\n",
    "    for sent in sentences:\n",
    "        input_ids = torch.tensor(tokenizer.encode(sent)).unsqueeze(0)\n",
    "        input_ids = input_ids.to('cuda')\n",
    "        outputs= model_text(input_ids)\n",
    "        cls_output = torch.mean(outputs[0], axis=1)\n",
    "        emb_set.append(cls_output)\n",
    "        if len(emb_set)>=20:\n",
    "            emb_set=emb_set[:20]\n",
    "        else:\n",
    "            deficit=20-len(emb_set)\n",
    "            for i in range(deficit):\n",
    "                emb_set.append(torch.zeros((1,768)).cuda())\n",
    "    emb_set = torch.cat(emb_set, dim=0)\n",
    "    emb_set= torch.flatten(emb_set,0).unsqueeze(0)\n",
    "    text_logist=text_classifer(emb_set)\n",
    "    return emb_set,text_logist\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15360]) torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "tex,_,_=train_dataset[0]\n",
    "emb,logist=text_embedding(tex)\n",
    "print(emb.size(),logist.size())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "import torchvision.transforms as transforms\n",
    "from model.resnet import resnet50\n",
    "transform_train = transforms.Compose([     \n",
    "#             transforms.Resize((256, 128), interpolation=3),\n",
    "        transforms.Resize((224,224), interpolation=3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "transform_test = transforms.Compose([     \n",
    "        transforms.Resize((224,224), interpolation=3),  # Image.BICUBIC\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "model_img = resnet50(pretrained=True,num_classes=2)\n",
    "model_img.to('cuda')\n",
    "def img_embedding(img_name,is_train, img_model_path=None):\n",
    "    if is_train:\n",
    "        img_path= 'AAAI_dataset/Images/politi_train/'+img_name\n",
    "        img = Image.open(img_path).convert('RGB')   \n",
    "        res_img=transform_train(img).float().unsqueeze(0).float()\n",
    "    else:\n",
    "        img_path= 'AAAI_dataset/Images/politi_test/'+img_name\n",
    "        img = Image.open(img_path).convert('RGB')   \n",
    "        res_img=transform_test(img).float().unsqueeze(0).float()   \n",
    "    if img_model_path:\n",
    "        checkpoint = torch.load(img_model_path)\n",
    "        model_img.load_state_dict(checkpoint['model'])\n",
    "    res_img=res_img.to('cuda')\n",
    "    img_feat,img_logist=model_img(res_img)\n",
    "    return img_feat,img_logist\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _,train_img,_=train_dataset[0]\n",
    "# _,test_img,_=test_dataset[0]\n",
    "# train_img_feat, train_img_logist=img_embedding(train_img,is_train=True)\n",
    "# test_img_feat, test_img_logist=img_embedding(test_img,is_train=False)\n",
    "# train_img_feat.size(), train_img_logist.size(), test_img_feat.size(), test_img_logist.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 768)\n",
       "    (layer): ModuleList(\n",
       "      (0): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (1): LinearEmbed(\n",
       "    (linear): Linear(in_features=15360, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from crd.criterion import CRDLoss\n",
    "import torch.optim as optim\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "criterion_cls=criterion_cls.cuda()\n",
    "\n",
    "optimizer_img = optim.SGD(model_img.parameters(), lr=0.0003, momentum=0.9, weight_decay=5e-4)\n",
    "optimizer_text = torch.optim.Adam(model_text.parameters(), lr=2e-5)   #1e-5, 2e-5, 3e-5\n",
    "\n",
    "text_module_list = nn.ModuleList([])\n",
    "text_module_list.append(model_text)\n",
    "text_module_list.append(text_classifer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# def img_embedding_batch(batch_data, is_train):\n",
    "#     batch_img_logist, batch_label=[],[]\n",
    "#     for idx, data in enumerate(batch_data):\n",
    "#         _, img_name, label=data\n",
    "#         _,img_item_logist = img_embedding(img_name,is_train=is_train)\n",
    "#         batch_img_logist.append(img_item_logist)\n",
    "#         batch_label.append(label)\n",
    "\n",
    "#     batch_img_logist=torch.cat(batch_img_logist,0)\n",
    "#     batch_label=torch.tensor(batch_label)\n",
    "    \n",
    "#     return batch_img_logist, batch_label\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *\n",
    "\n",
    "# def train_img_epoch(epoch, train_set):\n",
    "#     model_img.train()\n",
    "#     model_img.cuda()\n",
    "#     losses_img = AverageMeter()\n",
    "#     top1_img = AverageMeter()\n",
    "#     cur_step=0\n",
    "#     for i in range(0,384,16):  \n",
    "#         cur_step+=1\n",
    "#         if i<368:\n",
    "#             batch_data=train_set[i:i+16]\n",
    "#         else:\n",
    "#             batch_data=train_set[i:]\n",
    "#         batch_img_logist, batch_label = img_embedding_batch(batch_data, is_train=True)\n",
    "#         batch_label=batch_label.cuda()\n",
    "        \n",
    "#         loss_cls_img = criterion_cls(batch_img_logist, batch_label)\n",
    "\n",
    "#         acc1_img = accuracy(batch_img_logist, batch_label)\n",
    "#         losses_img.update(loss_cls_img.item(), batch_label.size(0))\n",
    "#         top1_img.update(float(acc1_img[0]), batch_label.size(0))\n",
    "\n",
    "#         optimizer_img.zero_grad()\n",
    "#         loss_cls_img.backward()\n",
    "#         optimizer_img.step()\n",
    "\n",
    "#         if cur_step % 3 == 0:\n",
    "#             print('Train for Image Model:')\n",
    "#             print('Epoch: [{0}]\\t'\n",
    "#                   'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "#                   'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t'.format(\n",
    "#                 epoch,\n",
    "#                 loss=losses_img, top1=top1_img))\n",
    "#             print('\\n')\n",
    "#     print(' Train image model : Acc@1 {top1.avg:.3f}'.format(top1=top1_img))\n",
    "    \n",
    "# def test_img_epoch(epoch, train_set):\n",
    "#     model_img.eval()\n",
    "#     model_img.cuda()\n",
    "#     losses_img = AverageMeter()\n",
    "#     top1_img = AverageMeter()\n",
    "#     cur_step=0\n",
    "#     with torch.no_grad():\n",
    "#         for i in range(0,112,16):  \n",
    "#             cur_step += 1\n",
    "#             if i<96:\n",
    "#                 batch_data=test_set[i:i+16]\n",
    "#             else:\n",
    "#                 batch_data=test_set[i:]\n",
    "    \n",
    "#             batch_img_logist, batch_label = img_embedding_batch(batch_data, is_train=False)\n",
    "#             batch_label=batch_label.cuda()\n",
    "\n",
    "#             loss_cls_img = criterion_cls(batch_img_logist, batch_label)\n",
    "\n",
    "#             acc1_img = accuracy(batch_img_logist, batch_label)\n",
    "#             losses_img.update(loss_cls_img.item(), batch_label.size(0))\n",
    "#             top1_img.update(float(acc1_img[0]), batch_label.size(0))\n",
    "\n",
    "            \n",
    "\n",
    "#             if cur_step % 3 == 0:\n",
    "#                 print('Test for Image Model:')\n",
    "#                 print('Epoch: [{0}]\\t'\n",
    "#                       'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "#                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t'.format(\n",
    "#                     epoch,\n",
    "#                     loss=losses_img, top1=top1_img))\n",
    "#                 print('\\n')\n",
    "#         print(' Test image model : Acc@1 {top1.avg:.3f}'.format(top1=top1_img))\n",
    "#         print('\\n')\n",
    "#         return top1_img.avg\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from conf import config\n",
    "# best_img_acc = 0\n",
    "# img_save_file = None\n",
    "# train_set=copy.deepcopy(train_dataset)\n",
    "# test_set=copy.deepcopy(test_dataset)\n",
    "# sys.stdout = Logger(os.path.join(config.save_folder, 'img.txt'))\n",
    "# for epoch_id in range(60):\n",
    "#     print('Epoch:', epoch_id)\n",
    "#     random.shuffle(train_set)\n",
    "#     train_img_epoch(epoch_id, train_set)\n",
    "#     test_img_acc = test_img_epoch(epoch_id,test_set)\n",
    "#     if test_img_acc > best_img_acc:\n",
    "#         best_img_acc = test_img_acc\n",
    "#         state = {\n",
    "#             'epoch': epoch_id,\n",
    "#             'model': model_img.state_dict(),\n",
    "#             'best_acc': best_img_acc,\n",
    "#         }\n",
    "#         img_save_file = os.path.join(config.save_folder, 'img_best.pth')\n",
    "#         print('saving the best model for img!')\n",
    "#         torch.save(state, img_save_file)\n",
    "#         print('\\n')\n",
    "# print('best accuracy for image model :', best_img_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_embedding_batch(batch_data):\n",
    "    batch_text_logist,batch_label=[],[]\n",
    "    for idx, data in enumerate(batch_data):\n",
    "        text, _, label=data\n",
    "        _,text_item_logist = text_embedding(text,text_model_path='/home/wzm/test/rumor_detect/data/log/xlnet-base-cased/politi/text_best.pth')\n",
    "        batch_text_logist.append(text_item_logist)\n",
    "        batch_label.append(label)\n",
    "\n",
    "    batch_text_logist=torch.cat(batch_text_logist,0)\n",
    "    batch_label=torch.tensor(batch_label)\n",
    "    \n",
    "    return batch_text_logist, batch_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *\n",
    "\n",
    "def train_text_epoch(epoch, train_set):\n",
    "    for module in text_module_list:\n",
    "        module.train()\n",
    "        module.cuda()\n",
    "    losses_text = AverageMeter()\n",
    "    top1_text = AverageMeter()\n",
    "    cur_step=0\n",
    "    for i in range(0,384,16):  \n",
    "        cur_step+=1\n",
    "        if i<368:\n",
    "            batch_data=train_set[i:i+16]\n",
    "        else:\n",
    "            batch_data=train_set[i:]\n",
    "        batch_text_logist, batch_label = text_embedding_batch(batch_data)\n",
    "        batch_label=batch_label.cuda()\n",
    "        \n",
    "        loss_cls_text = criterion_cls(batch_text_logist, batch_label)\n",
    "\n",
    "        acc1_text = accuracy(batch_text_logist, batch_label)\n",
    "        losses_text.update(loss_cls_text.item(), batch_label.size(0))\n",
    "        top1_text.update(float(acc1_text[0]), batch_label.size(0))\n",
    "\n",
    "        optimizer_text.zero_grad()\n",
    "        loss_cls_text.backward()\n",
    "        optimizer_text.step()\n",
    "\n",
    "        if cur_step % 3 == 0:\n",
    "            print('Train for Text Model:')\n",
    "            print('Epoch: [{0}]\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t'.format(\n",
    "                epoch,\n",
    "                loss=losses_text, top1=top1_text))\n",
    "            print('\\n')\n",
    "    print(' Train text model : Acc@1 {top1.avg:.3f}'.format(top1=top1_text))\n",
    "    \n",
    "def test_text_epoch(epoch, test_set):\n",
    "    for module in text_module_list:\n",
    "        module.eval()\n",
    "        module.cuda()\n",
    "    top1_text = AverageMeter()\n",
    "    count_0_lists = []\n",
    "    correct_0_lists = []\n",
    "    target_0_lists = []\n",
    "    count_1_lists = []\n",
    "    correct_1_lists = []\n",
    "    target_1_lists = []\n",
    "    cur_step=0\n",
    "    with torch.no_grad():\n",
    "        for i in range(0,112,16):  \n",
    "            cur_step += 1\n",
    "            if i<96:\n",
    "                batch_data=test_set[i:i+16]\n",
    "            else:\n",
    "                batch_data=test_set[i:]\n",
    "    \n",
    "            batch_text_logist, batch_label = text_embedding_batch(batch_data)\n",
    "            batch_label=batch_label.cuda()\n",
    "\n",
    "           \n",
    "\n",
    "            acc1_text = accuracy(batch_text_logist, batch_label)\n",
    "            \n",
    "            count_0, count_correct_0, count_target_0 = metric(logist_fc, batch_label, for_fake=True)\n",
    "            count_1, count_correct_1, count_target_1 = metric(logist_fc, batch_label, for_fake=False)\n",
    "            count_0_lists.append(count_0)\n",
    "            correct_0_lists.append(count_correct_0)\n",
    "            target_0_lists.append(count_target_0)\n",
    "\n",
    "            count_1_lists.append(count_1)\n",
    "            correct_1_lists.append(count_correct_1)\n",
    "            target_1_lists.append(count_target_1)\n",
    "\n",
    "           \n",
    "            top1_text.update(float(acc1_text[0]), batch_label.size(0))\n",
    "\n",
    "            \n",
    "\n",
    "            if cur_step % 3 == 0:\n",
    "                print('Test for Text Model:')\n",
    "                print('Epoch: [{0}]\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t'.format(\n",
    "                    epoch,\n",
    "                    loss=losses_text, top1=top1_text))\n",
    "                print('\\n')\n",
    "        epoch_count_0 = sum(count_0_lists)\n",
    "        epoch_correct_0 = sum(correct_0_lists)\n",
    "        epoch_target_0 = sum(target_0_lists)\n",
    "\n",
    "        epoch_count_1 = sum(count_1_lists)\n",
    "        epoch_correct_1 = sum(correct_1_lists)\n",
    "        epoch_target_1 = sum(target_1_lists)\n",
    "\n",
    "        try:\n",
    "            prec_0 = float(epoch_correct_0) * (100.0 / float(epoch_count_0))\n",
    "        except ZeroDivisionError:\n",
    "            prec_0 = 0\n",
    "        try:\n",
    "            rec_0 = float(epoch_correct_0) * (100.0 / float(epoch_target_0))\n",
    "        except ZeroDivisionError:\n",
    "            rec_0 = 0\n",
    "        try:\n",
    "            f_0 = 2 * (prec_0 * rec_0) / (prec_0 + rec_0)\n",
    "        except ZeroDivisionError:\n",
    "            f_0 = 0\n",
    "\n",
    "        try:\n",
    "            prec_1 = float(epoch_correct_1) * (100.0 / float(epoch_count_1))\n",
    "        except ZeroDivisionError:\n",
    "            prec_1 = 0\n",
    "        try:\n",
    "            rec_1 = float(epoch_correct_1) * (100.0 / float(epoch_target_1))\n",
    "        except ZeroDivisionError:\n",
    "            rec_1 = 0\n",
    "        try:\n",
    "            f_1 = 2 * (prec_1 * rec_1) / (prec_1 + rec_1)\n",
    "        except ZeroDivisionError:\n",
    "            f_1 = 0\n",
    "\n",
    "        # prec_1 = float(epoch_correct_1) * (100.0 / float(epoch_count_1))\n",
    "        # rec_1 = float(epoch_correct_1) * (100.0 / float(epoch_target_1))\n",
    "        # f_1 = 2 * (prec_1 * rec_1) / (prec_1 + rec_1)\n",
    "        print('val * Acc@1 {top1.avg:.3f} '.format(top1=top1_text))\n",
    "        print('metrics: prec_0, rec_0, f_0, prec_1, rec_1, f_1')\n",
    "        print(prec_0, rec_0, f_0, prec_1, rec_1, f_1)\n",
    "\n",
    "    return top1_text.avg, prec_0, rec_0, f_0, prec_1, rec_1, f_1\n",
    "        \n",
    "#         print(' Test text model : Acc@1 {top1.avg:.3f}'.format(top1=top1_text))\n",
    "#         print('\\n')\n",
    "#         return top1_text.avg\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for XLNetModel:\n\tMissing key(s) in state_dict: \"mask_emb\", \"word_embedding.weight\", \"layer.0.rel_attn.q\", \"layer.0.rel_attn.k\", \"layer.0.rel_attn.v\", \"layer.0.rel_attn.o\", \"layer.0.rel_attn.r\", \"layer.0.rel_attn.r_r_bias\", \"layer.0.rel_attn.r_s_bias\", \"layer.0.rel_attn.r_w_bias\", \"layer.0.rel_attn.seg_embed\", \"layer.0.rel_attn.layer_norm.weight\", \"layer.0.rel_attn.layer_norm.bias\", \"layer.0.ff.layer_norm.weight\", \"layer.0.ff.layer_norm.bias\", \"layer.0.ff.layer_1.weight\", \"layer.0.ff.layer_1.bias\", \"layer.0.ff.layer_2.weight\", \"layer.0.ff.layer_2.bias\", \"layer.1.rel_attn.q\", \"layer.1.rel_attn.k\", \"layer.1.rel_attn.v\", \"layer.1.rel_attn.o\", \"layer.1.rel_attn.r\", \"layer.1.rel_attn.r_r_bias\", \"layer.1.rel_attn.r_s_bias\", \"layer.1.rel_attn.r_w_bias\", \"layer.1.rel_attn.seg_embed\", \"layer.1.rel_attn.layer_norm.weight\", \"layer.1.rel_attn.layer_norm.bias\", \"layer.1.ff.layer_norm.weight\", \"layer.1.ff.layer_norm.bias\", \"layer.1.ff.layer_1.weight\", \"layer.1.ff.layer_1.bias\", \"layer.1.ff.layer_2.weight\", \"layer.1.ff.layer_2.bias\", \"layer.2.rel_attn.q\", \"layer.2.rel_attn.k\", \"layer.2.rel_attn.v\", \"layer.2.rel_attn.o\", \"layer.2.rel_attn.r\", \"layer.2.rel_attn.r_r_bias\", \"layer.2.rel_attn.r_s_bias\", \"layer.2.rel_attn.r_w_bias\", \"layer.2.rel_attn.seg_embed\", \"layer.2.rel_attn.layer_norm.weight\", \"layer.2.rel_attn.layer_norm.bias\", \"layer.2.ff.layer_norm.weight\", \"layer.2.ff.layer_norm.bias\", \"layer.2.ff.layer_1.weight\", \"layer.2.ff.layer_1.bias\", \"layer.2.ff.layer_2.weight\", \"layer.2.ff.layer_2.bias\", \"layer.3.rel_attn.q\", \"layer.3.rel_attn.k\", \"layer.3.rel_attn.v\", \"layer.3.rel_attn.o\", \"layer.3.rel_attn.r\", \"layer.3.rel_attn.r_r_bias\", \"layer.3.rel_attn.r_s_bias\", \"layer.3.rel_attn.r_w_bias\", \"layer.3.rel_attn.seg_embed\", \"layer.3.rel_attn.layer_norm.weight\", \"layer.3.rel_attn.layer_norm.bias\", \"layer.3.ff.layer_norm.weight\", \"layer.3.ff.layer_norm.bias\", \"layer.3.ff.layer_1.weight\", \"layer.3.ff.layer_1.bias\", \"layer.3.ff.layer_2.weight\", \"layer.3.ff.layer_2.bias\", \"layer.4.rel_attn.q\", \"layer.4.rel_attn.k\", \"layer.4.rel_attn.v\", \"layer.4.rel_attn.o\", \"layer.4.rel_attn.r\", \"layer.4.rel_attn.r_r_bias\", \"layer.4.rel_attn.r_s_bias\", \"layer.4.rel_attn.r_w_bias\", \"layer.4.rel_attn.seg_embed\", \"layer.4.rel_attn.layer_norm.weight\", \"layer.4.rel_attn.layer_norm.bias\", \"layer.4.ff.layer_norm.weight\", \"layer.4.ff.layer_norm.bias\", \"layer.4.ff.layer_1.weight\", \"layer.4.ff.layer_1.bias\", \"layer.4.ff.layer_2.weight\", \"layer.4.ff.layer_2.bias\", \"layer.5.rel_attn.q\", \"layer.5.rel_attn.k\", \"layer.5.rel_attn.v\", \"layer.5.rel_attn.o\", \"layer.5.rel_attn.r\", \"layer.5.rel_attn.r_r_bias\", \"layer.5.rel_attn.r_s_bias\", \"layer.5.rel_attn.r_w_bias\", \"layer.5.rel_attn.seg_embed\", \"layer.5.rel_attn.layer_norm.weight\", \"layer.5.rel_attn.layer_norm.bias\", \"layer.5.ff.layer_norm.weight\", \"layer.5.ff.layer_norm.bias\", \"layer.5.ff.layer_1.weight\", \"layer.5.ff.layer_1.bias\", \"layer.5.ff.layer_2.weight\", \"layer.5.ff.layer_2.bias\", \"layer.6.rel_attn.q\", \"layer.6.rel_attn.k\", \"layer.6.rel_attn.v\", \"layer.6.rel_attn.o\", \"layer.6.rel_attn.r\", \"layer.6.rel_attn.r_r_bias\", \"layer.6.rel_attn.r_s_bias\", \"layer.6.rel_attn.r_w_bias\", \"layer.6.rel_attn.seg_embed\", \"layer.6.rel_attn.layer_norm.weight\", \"layer.6.rel_attn.layer_norm.bias\", \"layer.6.ff.layer_norm.weight\", \"layer.6.ff.layer_norm.bias\", \"layer.6.ff.layer_1.weight\", \"layer.6.ff.layer_1.bias\", \"layer.6.ff.layer_2.weight\", \"layer.6.ff.layer_2.bias\", \"layer.7.rel_attn.q\", \"layer.7.rel_attn.k\", \"layer.7.rel_attn.v\", \"layer.7.rel_attn.o\", \"layer.7.rel_attn.r\", \"layer.7.rel_attn.r_r_bias\", \"layer.7.rel_attn.r_s_bias\", \"layer.7.rel_attn.r_w_bias\", \"layer.7.rel_attn.seg_embed\", \"layer.7.rel_attn.layer_norm.weight\", \"layer.7.rel_attn.layer_norm.bias\", \"layer.7.ff.layer_norm.weight\", \"layer.7.ff.layer_norm.bias\", \"layer.7.ff.layer_1.weight\", \"layer.7.ff.layer_1.bias\", \"layer.7.ff.layer_2.weight\", \"layer.7.ff.layer_2.bias\", \"layer.8.rel_attn.q\", \"layer.8.rel_attn.k\", \"layer.8.rel_attn.v\", \"layer.8.rel_attn.o\", \"layer.8.rel_attn.r\", \"layer.8.rel_attn.r_r_bias\", \"layer.8.rel_attn.r_s_bias\", \"layer.8.rel_attn.r_w_bias\", \"layer.8.rel_attn.seg_embed\", \"layer.8.rel_attn.layer_norm.weight\", \"layer.8.rel_attn.layer_norm.bias\", \"layer.8.ff.layer_norm.weight\", \"layer.8.ff.layer_norm.bias\", \"layer.8.ff.layer_1.weight\", \"layer.8.ff.layer_1.bias\", \"layer.8.ff.layer_2.weight\", \"layer.8.ff.layer_2.bias\", \"layer.9.rel_attn.q\", \"layer.9.rel_attn.k\", \"layer.9.rel_attn.v\", \"layer.9.rel_attn.o\", \"layer.9.rel_attn.r\", \"layer.9.rel_attn.r_r_bias\", \"layer.9.rel_attn.r_s_bias\", \"layer.9.rel_attn.r_w_bias\", \"layer.9.rel_attn.seg_embed\", \"layer.9.rel_attn.layer_norm.weight\", \"layer.9.rel_attn.layer_norm.bias\", \"layer.9.ff.layer_norm.weight\", \"layer.9.ff.layer_norm.bias\", \"layer.9.ff.layer_1.weight\", \"layer.9.ff.layer_1.bias\", \"layer.9.ff.layer_2.weight\", \"layer.9.ff.layer_2.bias\", \"layer.10.rel_attn.q\", \"layer.10.rel_attn.k\", \"layer.10.rel_attn.v\", \"layer.10.rel_attn.o\", \"layer.10.rel_attn.r\", \"layer.10.rel_attn.r_r_bias\", \"layer.10.rel_attn.r_s_bias\", \"layer.10.rel_attn.r_w_bias\", \"layer.10.rel_attn.seg_embed\", \"layer.10.rel_attn.layer_norm.weight\", \"layer.10.rel_attn.layer_norm.bias\", \"layer.10.ff.layer_norm.weight\", \"layer.10.ff.layer_norm.bias\", \"layer.10.ff.layer_1.weight\", \"layer.10.ff.layer_1.bias\", \"layer.10.ff.layer_2.weight\", \"layer.10.ff.layer_2.bias\", \"layer.11.rel_attn.q\", \"layer.11.rel_attn.k\", \"layer.11.rel_attn.v\", \"layer.11.rel_attn.o\", \"layer.11.rel_attn.r\", \"layer.11.rel_attn.r_r_bias\", \"layer.11.rel_attn.r_s_bias\", \"layer.11.rel_attn.r_w_bias\", \"layer.11.rel_attn.seg_embed\", \"layer.11.rel_attn.layer_norm.weight\", \"layer.11.rel_attn.layer_norm.bias\", \"layer.11.ff.layer_norm.weight\", \"layer.11.ff.layer_norm.bias\", \"layer.11.ff.layer_1.weight\", \"layer.11.ff.layer_1.bias\", \"layer.11.ff.layer_2.weight\", \"layer.11.ff.layer_2.bias\". \n\tUnexpected key(s) in state_dict: \"model.mask_emb\", \"model.word_embedding.weight\", \"model.layer.0.rel_attn.q\", \"model.layer.0.rel_attn.k\", \"model.layer.0.rel_attn.v\", \"model.layer.0.rel_attn.o\", \"model.layer.0.rel_attn.r\", \"model.layer.0.rel_attn.r_r_bias\", \"model.layer.0.rel_attn.r_s_bias\", \"model.layer.0.rel_attn.r_w_bias\", \"model.layer.0.rel_attn.seg_embed\", \"model.layer.0.rel_attn.layer_norm.weight\", \"model.layer.0.rel_attn.layer_norm.bias\", \"model.layer.0.ff.layer_norm.weight\", \"model.layer.0.ff.layer_norm.bias\", \"model.layer.0.ff.layer_1.weight\", \"model.layer.0.ff.layer_1.bias\", \"model.layer.0.ff.layer_2.weight\", \"model.layer.0.ff.layer_2.bias\", \"model.layer.1.rel_attn.q\", \"model.layer.1.rel_attn.k\", \"model.layer.1.rel_attn.v\", \"model.layer.1.rel_attn.o\", \"model.layer.1.rel_attn.r\", \"model.layer.1.rel_attn.r_r_bias\", \"model.layer.1.rel_attn.r_s_bias\", \"model.layer.1.rel_attn.r_w_bias\", \"model.layer.1.rel_attn.seg_embed\", \"model.layer.1.rel_attn.layer_norm.weight\", \"model.layer.1.rel_attn.layer_norm.bias\", \"model.layer.1.ff.layer_norm.weight\", \"model.layer.1.ff.layer_norm.bias\", \"model.layer.1.ff.layer_1.weight\", \"model.layer.1.ff.layer_1.bias\", \"model.layer.1.ff.layer_2.weight\", \"model.layer.1.ff.layer_2.bias\", \"model.layer.2.rel_attn.q\", \"model.layer.2.rel_attn.k\", \"model.layer.2.rel_attn.v\", \"model.layer.2.rel_attn.o\", \"model.layer.2.rel_attn.r\", \"model.layer.2.rel_attn.r_r_bias\", \"model.layer.2.rel_attn.r_s_bias\", \"model.layer.2.rel_attn.r_w_bias\", \"model.layer.2.rel_attn.seg_embed\", \"model.layer.2.rel_attn.layer_norm.weight\", \"model.layer.2.rel_attn.layer_norm.bias\", \"model.layer.2.ff.layer_norm.weight\", \"model.layer.2.ff.layer_norm.bias\", \"model.layer.2.ff.layer_1.weight\", \"model.layer.2.ff.layer_1.bias\", \"model.layer.2.ff.layer_2.weight\", \"model.layer.2.ff.layer_2.bias\", \"model.layer.3.rel_attn.q\", \"model.layer.3.rel_attn.k\", \"model.layer.3.rel_attn.v\", \"model.layer.3.rel_attn.o\", \"model.layer.3.rel_attn.r\", \"model.layer.3.rel_attn.r_r_bias\", \"model.layer.3.rel_attn.r_s_bias\", \"model.layer.3.rel_attn.r_w_bias\", \"model.layer.3.rel_attn.seg_embed\", \"model.layer.3.rel_attn.layer_norm.weight\", \"model.layer.3.rel_attn.layer_norm.bias\", \"model.layer.3.ff.layer_norm.weight\", \"model.layer.3.ff.layer_norm.bias\", \"model.layer.3.ff.layer_1.weight\", \"model.layer.3.ff.layer_1.bias\", \"model.layer.3.ff.layer_2.weight\", \"model.layer.3.ff.layer_2.bias\", \"model.layer.4.rel_attn.q\", \"model.layer.4.rel_attn.k\", \"model.layer.4.rel_attn.v\", \"model.layer.4.rel_attn.o\", \"model.layer.4.rel_attn.r\", \"model.layer.4.rel_attn.r_r_bias\", \"model.layer.4.rel_attn.r_s_bias\", \"model.layer.4.rel_attn.r_w_bias\", \"model.layer.4.rel_attn.seg_embed\", \"model.layer.4.rel_attn.layer_norm.weight\", \"model.layer.4.rel_attn.layer_norm.bias\", \"model.layer.4.ff.layer_norm.weight\", \"model.layer.4.ff.layer_norm.bias\", \"model.layer.4.ff.layer_1.weight\", \"model.layer.4.ff.layer_1.bias\", \"model.layer.4.ff.layer_2.weight\", \"model.layer.4.ff.layer_2.bias\", \"model.layer.5.rel_attn.q\", \"model.layer.5.rel_attn.k\", \"model.layer.5.rel_attn.v\", \"model.layer.5.rel_attn.o\", \"model.layer.5.rel_attn.r\", \"model.layer.5.rel_attn.r_r_bias\", \"model.layer.5.rel_attn.r_s_bias\", \"model.layer.5.rel_attn.r_w_bias\", \"model.layer.5.rel_attn.seg_embed\", \"model.layer.5.rel_attn.layer_norm.weight\", \"model.layer.5.rel_attn.layer_norm.bias\", \"model.layer.5.ff.layer_norm.weight\", \"model.layer.5.ff.layer_norm.bias\", \"model.layer.5.ff.layer_1.weight\", \"model.layer.5.ff.layer_1.bias\", \"model.layer.5.ff.layer_2.weight\", \"model.layer.5.ff.layer_2.bias\", \"model.layer.6.rel_attn.q\", \"model.layer.6.rel_attn.k\", \"model.layer.6.rel_attn.v\", \"model.layer.6.rel_attn.o\", \"model.layer.6.rel_attn.r\", \"model.layer.6.rel_attn.r_r_bias\", \"model.layer.6.rel_attn.r_s_bias\", \"model.layer.6.rel_attn.r_w_bias\", \"model.layer.6.rel_attn.seg_embed\", \"model.layer.6.rel_attn.layer_norm.weight\", \"model.layer.6.rel_attn.layer_norm.bias\", \"model.layer.6.ff.layer_norm.weight\", \"model.layer.6.ff.layer_norm.bias\", \"model.layer.6.ff.layer_1.weight\", \"model.layer.6.ff.layer_1.bias\", \"model.layer.6.ff.layer_2.weight\", \"model.layer.6.ff.layer_2.bias\", \"model.layer.7.rel_attn.q\", \"model.layer.7.rel_attn.k\", \"model.layer.7.rel_attn.v\", \"model.layer.7.rel_attn.o\", \"model.layer.7.rel_attn.r\", \"model.layer.7.rel_attn.r_r_bias\", \"model.layer.7.rel_attn.r_s_bias\", \"model.layer.7.rel_attn.r_w_bias\", \"model.layer.7.rel_attn.seg_embed\", \"model.layer.7.rel_attn.layer_norm.weight\", \"model.layer.7.rel_attn.layer_norm.bias\", \"model.layer.7.ff.layer_norm.weight\", \"model.layer.7.ff.layer_norm.bias\", \"model.layer.7.ff.layer_1.weight\", \"model.layer.7.ff.layer_1.bias\", \"model.layer.7.ff.layer_2.weight\", \"model.layer.7.ff.layer_2.bias\", \"model.layer.8.rel_attn.q\", \"model.layer.8.rel_attn.k\", \"model.layer.8.rel_attn.v\", \"model.layer.8.rel_attn.o\", \"model.layer.8.rel_attn.r\", \"model.layer.8.rel_attn.r_r_bias\", \"model.layer.8.rel_attn.r_s_bias\", \"model.layer.8.rel_attn.r_w_bias\", \"model.layer.8.rel_attn.seg_embed\", \"model.layer.8.rel_attn.layer_norm.weight\", \"model.layer.8.rel_attn.layer_norm.bias\", \"model.layer.8.ff.layer_norm.weight\", \"model.layer.8.ff.layer_norm.bias\", \"model.layer.8.ff.layer_1.weight\", \"model.layer.8.ff.layer_1.bias\", \"model.layer.8.ff.layer_2.weight\", \"model.layer.8.ff.layer_2.bias\", \"model.layer.9.rel_attn.q\", \"model.layer.9.rel_attn.k\", \"model.layer.9.rel_attn.v\", \"model.layer.9.rel_attn.o\", \"model.layer.9.rel_attn.r\", \"model.layer.9.rel_attn.r_r_bias\", \"model.layer.9.rel_attn.r_s_bias\", \"model.layer.9.rel_attn.r_w_bias\", \"model.layer.9.rel_attn.seg_embed\", \"model.layer.9.rel_attn.layer_norm.weight\", \"model.layer.9.rel_attn.layer_norm.bias\", \"model.layer.9.ff.layer_norm.weight\", \"model.layer.9.ff.layer_norm.bias\", \"model.layer.9.ff.layer_1.weight\", \"model.layer.9.ff.layer_1.bias\", \"model.layer.9.ff.layer_2.weight\", \"model.layer.9.ff.layer_2.bias\", \"model.layer.10.rel_attn.q\", \"model.layer.10.rel_attn.k\", \"model.layer.10.rel_attn.v\", \"model.layer.10.rel_attn.o\", \"model.layer.10.rel_attn.r\", \"model.layer.10.rel_attn.r_r_bias\", \"model.layer.10.rel_attn.r_s_bias\", \"model.layer.10.rel_attn.r_w_bias\", \"model.layer.10.rel_attn.seg_embed\", \"model.layer.10.rel_attn.layer_norm.weight\", \"model.layer.10.rel_attn.layer_norm.bias\", \"model.layer.10.ff.layer_norm.weight\", \"model.layer.10.ff.layer_norm.bias\", \"model.layer.10.ff.layer_1.weight\", \"model.layer.10.ff.layer_1.bias\", \"model.layer.10.ff.layer_2.weight\", \"model.layer.10.ff.layer_2.bias\", \"model.layer.11.rel_attn.q\", \"model.layer.11.rel_attn.k\", \"model.layer.11.rel_attn.v\", \"model.layer.11.rel_attn.o\", \"model.layer.11.rel_attn.r\", \"model.layer.11.rel_attn.r_r_bias\", \"model.layer.11.rel_attn.r_s_bias\", \"model.layer.11.rel_attn.r_w_bias\", \"model.layer.11.rel_attn.seg_embed\", \"model.layer.11.rel_attn.layer_norm.weight\", \"model.layer.11.rel_attn.layer_norm.bias\", \"model.layer.11.ff.layer_norm.weight\", \"model.layer.11.ff.layer_norm.bias\", \"model.layer.11.ff.layer_1.weight\", \"model.layer.11.ff.layer_1.bias\", \"model.layer.11.ff.layer_2.weight\", \"model.layer.11.ff.layer_2.bias\", \"classifer.weight\", \"classifer.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-fcc491164685>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_text_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'full metric under best accuracy: acc, prec, rec, F1, prec_fake, rec_fake, F1_fake'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-aee9319705fc>\u001b[0m in \u001b[0;36mtest_text_epoch\u001b[0;34m(epoch, test_set)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mbatch_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mbatch_text_logist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_embedding_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mbatch_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-9e65035880eb>\u001b[0m in \u001b[0;36mtext_embedding_batch\u001b[0;34m(batch_data)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext_item_logist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext_model_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/home/wzm/test/rumor_detect/data/log/xlnet-base-cased/politi/text_best.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mbatch_text_logist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_item_logist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mbatch_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-50d2e816b25d>\u001b[0m in \u001b[0;36mtext_embedding\u001b[0;34m(text, text_model_path)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mcheckpoint_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mmodel_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 830\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    831\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for XLNetModel:\n\tMissing key(s) in state_dict: \"mask_emb\", \"word_embedding.weight\", \"layer.0.rel_attn.q\", \"layer.0.rel_attn.k\", \"layer.0.rel_attn.v\", \"layer.0.rel_attn.o\", \"layer.0.rel_attn.r\", \"layer.0.rel_attn.r_r_bias\", \"layer.0.rel_attn.r_s_bias\", \"layer.0.rel_attn.r_w_bias\", \"layer.0.rel_attn.seg_embed\", \"layer.0.rel_attn.layer_norm.weight\", \"layer.0.rel_attn.layer_norm.bias\", \"layer.0.ff.layer_norm.weight\", \"layer.0.ff.layer_norm.bias\", \"layer.0.ff.layer_1.weight\", \"layer.0.ff.layer_1.bias\", \"layer.0.ff.layer_2.weight\", \"layer.0.ff.layer_2.bias\", \"layer.1.rel_attn.q\", \"layer.1.rel_attn.k\", \"layer.1.rel_attn.v\", \"layer.1.rel_attn.o\", \"layer.1.rel_attn.r\", \"layer.1.rel_attn.r_r_bias\", \"layer.1.rel_attn.r_s_bias\", \"layer.1.rel_attn.r_w_bias\", \"layer.1.rel_attn.seg_embed\", \"layer.1.rel_attn.layer_norm.weight\", \"layer.1.rel_attn.layer_norm.bias\", \"layer.1.ff.layer_norm.weight\", \"layer.1.ff.layer_norm.bias\", \"layer.1.ff.layer_1.weight\", \"layer.1.ff.layer_1.bias\", \"layer.1.ff.layer_2.weight\", \"layer.1.ff.layer_2.bias\", \"layer.2.rel_attn.q\", \"layer.2.rel_attn.k\", \"layer.2.rel_attn.v\", \"layer.2.rel_attn.o\", \"layer.2.rel_attn.r\", \"layer.2.rel_attn.r_r_bias\", \"layer.2.rel_attn.r_s_bias\", \"layer.2.rel_attn.r_w_bias\", \"layer.2.rel_attn.seg_embed\", \"layer.2.rel_attn.layer_norm.weight\", \"layer.2.rel_attn.layer_norm.bias\", \"layer.2.ff.layer_norm.weight\", \"layer.2.ff.layer_norm.bias\", \"layer.2.ff.layer_1.weight\", \"layer.2.ff.layer_1.bias\", \"layer.2.ff.layer_2.weight\", \"layer.2.ff.layer_2.bias\", \"layer.3.rel_attn.q\", \"layer.3.rel_attn.k\", \"layer.3.rel_attn.v\", \"layer.3.rel_attn.o\", \"layer.3.rel_attn.r\", \"layer.3.rel_attn.r_r_bias\", \"layer.3.rel_attn.r_s_bias\", \"layer.3.rel_attn.r_w_bias\", \"layer.3.rel_attn.seg_embed\", \"layer.3.rel_attn.layer_norm.weight\", \"layer.3.rel_attn.layer_norm.bias\", \"layer.3.ff.layer_norm.weight\", \"layer.3.ff.layer_norm.bias\", \"layer.3.ff.layer_1.weight\", \"layer.3.ff.layer_1.bias\", \"layer.3.ff.layer_2.weight\", \"layer.3.ff.layer_2.bias\", \"layer.4.rel_attn.q\", \"layer.4.rel_attn.k\", \"layer.4.rel_attn.v\", \"layer.4.rel_attn.o\", \"layer.4.rel_attn.r\", \"layer.4.rel_attn.r_r_bias\", \"layer.4.rel_attn.r_s_bias\", \"layer.4.rel_attn.r_w_bias\", \"layer.4.rel_attn.seg_embed\", \"layer.4.rel_attn.layer_norm.weight\", \"layer.4.rel_attn.layer_norm.bias\", \"layer.4.ff.layer_norm.weight\", \"layer.4.ff.layer_norm.bias\", \"layer.4.ff.layer_1.weight\", \"layer.4.ff.layer_1.bias\", \"layer.4.ff.layer_2.weight\", \"layer.4.ff.layer_2.bias\", \"layer.5.rel_attn.q\", \"layer.5.rel_attn.k\", \"layer.5.rel_attn.v\", \"layer.5.rel_attn.o\", \"layer.5.rel_attn.r\", \"layer.5.rel_attn.r_r_bias\", \"layer.5.rel_attn.r_s_bias\", \"layer.5.rel_attn.r_w_bias\", \"layer.5.rel_attn.seg_embed\", \"layer.5.rel_attn.layer_norm.weight\", \"layer.5.rel_attn.layer_norm.bias\", \"layer.5.ff.layer_norm.weight\", \"layer.5.ff.layer_norm.bias\", \"layer.5.ff.layer_1.weight\", \"layer.5.ff.layer_1.bias\", \"layer.5.ff.layer_2.weight\", \"layer.5.ff.layer_2.bias\", \"layer.6.rel_attn.q\", \"layer.6.rel_attn.k\", \"layer.6.rel_attn.v\", \"layer.6.rel_attn.o\", \"layer.6.rel_attn.r\", \"layer.6.rel_attn.r_r_bias\", \"layer.6.rel_attn.r_s_bias\", \"layer.6.rel_attn.r_w_bias\", \"layer.6.rel_attn.seg_embed\", \"layer.6.rel_attn.layer_norm.weight\", \"layer.6.rel_attn.layer_norm.bias\", \"layer.6.ff.layer_norm.weight\", \"layer.6.ff.layer_norm.bias\", \"layer.6.ff.layer_1.weight\", \"layer.6.ff.layer_1.bias\", \"layer.6.ff.layer_2.weight\", \"layer.6.ff.layer_2.bias\", \"layer.7.rel_attn.q\", \"layer.7.rel_attn.k\", \"layer.7.rel_attn.v\", \"layer.7.rel_attn.o\", \"layer.7.rel_attn.r\", \"layer.7.rel_attn.r_r_bias\", \"layer.7.rel_attn.r_s_bias\", \"layer.7.rel_attn.r_w_bias\", \"layer.7.rel_attn.seg_embed\", \"layer.7.rel_attn.layer_norm.weight\", \"layer.7.rel_attn.layer_norm.bias\", \"layer.7.ff.layer_norm.weight\", \"layer.7.ff.layer_norm.bias\", \"layer.7.ff.layer_1.weight\", \"layer.7.ff.layer_1.bias\", \"layer.7.ff.layer_2.weight\", \"layer.7.ff.layer_2.bias\", \"layer.8.rel_attn.q\", \"layer.8.rel_attn.k\", \"layer.8.rel_attn.v\", \"layer.8.rel_attn.o\", \"layer.8.rel_attn.r\", \"layer.8.rel_attn.r_r_bias\", \"layer.8.rel_attn.r_s_bias\", \"layer.8.rel_attn.r_w_bias\", \"layer.8.rel_attn.seg_embed\", \"layer.8.rel_attn.layer_norm.weight\", \"layer.8.rel_attn.layer_norm.bias\", \"layer.8.ff.layer_norm.weight\", \"layer.8.ff.layer_norm.bias\", \"layer.8.ff.layer_1.weight\", \"layer.8.ff.layer_1.bias\", \"layer.8.ff.layer_2.weight\", \"layer.8.ff.layer_2.bias\", \"layer.9.rel_attn.q\", \"layer.9.rel_attn.k\", \"layer.9.rel_attn.v\", \"layer.9.rel_attn.o\", \"layer.9.rel_attn.r\", \"layer.9.rel_attn.r_r_bias\", \"layer.9.rel_attn.r_s_bias\", \"layer.9.rel_attn.r_w_bias\", \"layer.9.rel_attn.seg_embed\", \"layer.9.rel_attn.layer_norm.weight\", \"layer.9.rel_attn.layer_norm.bias\", \"layer.9.ff.layer_norm.weight\", \"layer.9.ff.layer_norm.bias\", \"layer.9.ff.layer_1.weight\", \"layer.9.ff.layer_1.bias\", \"layer.9.ff.layer_2.weight\", \"layer.9.ff.layer_2.bias\", \"layer.10.rel_attn.q\", \"layer.10.rel_attn.k\", \"layer.10.rel_attn.v\", \"layer.10.rel_attn.o\", \"layer.10.rel_attn.r\", \"layer.10.rel_attn.r_r_bias\", \"layer.10.rel_attn.r_s_bias\", \"layer.10.rel_attn.r_w_bias\", \"layer.10.rel_attn.seg_embed\", \"layer.10.rel_attn.layer_norm.weight\", \"layer.10.rel_attn.layer_norm.bias\", \"layer.10.ff.layer_norm.weight\", \"layer.10.ff.layer_norm.bias\", \"layer.10.ff.layer_1.weight\", \"layer.10.ff.layer_1.bias\", \"layer.10.ff.layer_2.weight\", \"layer.10.ff.layer_2.bias\", \"layer.11.rel_attn.q\", \"layer.11.rel_attn.k\", \"layer.11.rel_attn.v\", \"layer.11.rel_attn.o\", \"layer.11.rel_attn.r\", \"layer.11.rel_attn.r_r_bias\", \"layer.11.rel_attn.r_s_bias\", \"layer.11.rel_attn.r_w_bias\", \"layer.11.rel_attn.seg_embed\", \"layer.11.rel_attn.layer_norm.weight\", \"layer.11.rel_attn.layer_norm.bias\", \"layer.11.ff.layer_norm.weight\", \"layer.11.ff.layer_norm.bias\", \"layer.11.ff.layer_1.weight\", \"layer.11.ff.layer_1.bias\", \"layer.11.ff.layer_2.weight\", \"layer.11.ff.layer_2.bias\". \n\tUnexpected key(s) in state_dict: \"model.mask_emb\", \"model.word_embedding.weight\", \"model.layer.0.rel_attn.q\", \"model.layer.0.rel_attn.k\", \"model.layer.0.rel_attn.v\", \"model.layer.0.rel_attn.o\", \"model.layer.0.rel_attn.r\", \"model.layer.0.rel_attn.r_r_bias\", \"model.layer.0.rel_attn.r_s_bias\", \"model.layer.0.rel_attn.r_w_bias\", \"model.layer.0.rel_attn.seg_embed\", \"model.layer.0.rel_attn.layer_norm.weight\", \"model.layer.0.rel_attn.layer_norm.bias\", \"model.layer.0.ff.layer_norm.weight\", \"model.layer.0.ff.layer_norm.bias\", \"model.layer.0.ff.layer_1.weight\", \"model.layer.0.ff.layer_1.bias\", \"model.layer.0.ff.layer_2.weight\", \"model.layer.0.ff.layer_2.bias\", \"model.layer.1.rel_attn.q\", \"model.layer.1.rel_attn.k\", \"model.layer.1.rel_attn.v\", \"model.layer.1.rel_attn.o\", \"model.layer.1.rel_attn.r\", \"model.layer.1.rel_attn.r_r_bias\", \"model.layer.1.rel_attn.r_s_bias\", \"model.layer.1.rel_attn.r_w_bias\", \"model.layer.1.rel_attn.seg_embed\", \"model.layer.1.rel_attn.layer_norm.weight\", \"model.layer.1.rel_attn.layer_norm.bias\", \"model.layer.1.ff.layer_norm.weight\", \"model.layer.1.ff.layer_norm.bias\", \"model.layer.1.ff.layer_1.weight\", \"model.layer.1.ff.layer_1.bias\", \"model.layer.1.ff.layer_2.weight\", \"model.layer.1.ff.layer_2.bias\", \"model.layer.2.rel_attn.q\", \"model.layer.2.rel_attn.k\", \"model.layer.2.rel_attn.v\", \"model.layer.2.rel_attn.o\", \"model.layer.2.rel_attn.r\", \"model.layer.2.rel_attn.r_r_bias\", \"model.layer.2.rel_attn.r_s_bias\", \"model.layer.2.rel_attn.r_w_bias\", \"model.layer.2.rel_attn.seg_embed\", \"model.layer.2.rel_attn.layer_norm.weight\", \"model.layer.2.rel_attn.layer_norm.bias\", \"model.layer.2.ff.layer_norm.weight\", \"model.layer.2.ff.layer_norm.bias\", \"model.layer.2.ff.layer_1.weight\", \"model.layer.2.ff.layer_1.bias\", \"model.layer.2.ff.layer_2.weight\", \"model.layer.2.ff.layer_2.bias\", \"model.layer.3.rel_attn.q\", \"model.layer.3.rel_attn.k\", \"model.layer.3.rel_attn.v\", \"model.layer.3.rel_attn.o\", \"model.layer.3.rel_attn.r\", \"model.layer.3.rel_attn.r_r_bias\", \"model.layer.3.rel_attn.r_s_bias\", \"model.layer.3.rel_attn.r_w_bias\", \"model.layer.3.rel_attn.seg_embed\", \"model.layer.3.rel_attn.layer_norm.weight\", \"model.layer.3.rel_attn.layer_norm.bias\", \"model.layer.3.ff.layer_norm.weight\", \"model.layer.3.ff.layer_norm.bias\", \"model.layer.3.ff.layer_1.weight\", \"model.layer.3.ff.layer_1.bias\", \"model.layer.3.ff.layer_2.weight\", \"model.layer.3.ff.layer_2.bias\", \"model.layer.4.rel_attn.q\", \"model.layer.4.rel_attn.k\", \"model.layer.4.rel_attn.v\", \"model.layer.4.rel_attn.o\", \"model.layer.4.rel_attn.r\", \"model.layer.4.rel_attn.r_r_bias\", \"model.layer.4.rel_attn.r_s_bias\", \"model.layer.4.rel_attn.r_w_bias\", \"model.layer.4.rel_attn.seg_embed\", \"model.layer.4.rel_attn.layer_norm.weight\", \"model.layer.4.rel_attn.layer_norm.bias\", \"model.layer.4.ff.layer_norm.weight\", \"model.layer.4.ff.layer_norm.bias\", \"model.layer.4.ff.layer_1.weight\", \"model.layer.4.ff.layer_1.bias\", \"model.layer.4.ff.layer_2.weight\", \"model.layer.4.ff.layer_2.bias\", \"model.layer.5.rel_attn.q\", \"model.layer.5.rel_attn.k\", \"model.layer.5.rel_attn.v\", \"model.layer.5.rel_attn.o\", \"model.layer.5.rel_attn.r\", \"model.layer.5.rel_attn.r_r_bias\", \"model.layer.5.rel_attn.r_s_bias\", \"model.layer.5.rel_attn.r_w_bias\", \"model.layer.5.rel_attn.seg_embed\", \"model.layer.5.rel_attn.layer_norm.weight\", \"model.layer.5.rel_attn.layer_norm.bias\", \"model.layer.5.ff.layer_norm.weight\", \"model.layer.5.ff.layer_norm.bias\", \"model.layer.5.ff.layer_1.weight\", \"model.layer.5.ff.layer_1.bias\", \"model.layer.5.ff.layer_2.weight\", \"model.layer.5.ff.layer_2.bias\", \"model.layer.6.rel_attn.q\", \"model.layer.6.rel_attn.k\", \"model.layer.6.rel_attn.v\", \"model.layer.6.rel_attn.o\", \"model.layer.6.rel_attn.r\", \"model.layer.6.rel_attn.r_r_bias\", \"model.layer.6.rel_attn.r_s_bias\", \"model.layer.6.rel_attn.r_w_bias\", \"model.layer.6.rel_attn.seg_embed\", \"model.layer.6.rel_attn.layer_norm.weight\", \"model.layer.6.rel_attn.layer_norm.bias\", \"model.layer.6.ff.layer_norm.weight\", \"model.layer.6.ff.layer_norm.bias\", \"model.layer.6.ff.layer_1.weight\", \"model.layer.6.ff.layer_1.bias\", \"model.layer.6.ff.layer_2.weight\", \"model.layer.6.ff.layer_2.bias\", \"model.layer.7.rel_attn.q\", \"model.layer.7.rel_attn.k\", \"model.layer.7.rel_attn.v\", \"model.layer.7.rel_attn.o\", \"model.layer.7.rel_attn.r\", \"model.layer.7.rel_attn.r_r_bias\", \"model.layer.7.rel_attn.r_s_bias\", \"model.layer.7.rel_attn.r_w_bias\", \"model.layer.7.rel_attn.seg_embed\", \"model.layer.7.rel_attn.layer_norm.weight\", \"model.layer.7.rel_attn.layer_norm.bias\", \"model.layer.7.ff.layer_norm.weight\", \"model.layer.7.ff.layer_norm.bias\", \"model.layer.7.ff.layer_1.weight\", \"model.layer.7.ff.layer_1.bias\", \"model.layer.7.ff.layer_2.weight\", \"model.layer.7.ff.layer_2.bias\", \"model.layer.8.rel_attn.q\", \"model.layer.8.rel_attn.k\", \"model.layer.8.rel_attn.v\", \"model.layer.8.rel_attn.o\", \"model.layer.8.rel_attn.r\", \"model.layer.8.rel_attn.r_r_bias\", \"model.layer.8.rel_attn.r_s_bias\", \"model.layer.8.rel_attn.r_w_bias\", \"model.layer.8.rel_attn.seg_embed\", \"model.layer.8.rel_attn.layer_norm.weight\", \"model.layer.8.rel_attn.layer_norm.bias\", \"model.layer.8.ff.layer_norm.weight\", \"model.layer.8.ff.layer_norm.bias\", \"model.layer.8.ff.layer_1.weight\", \"model.layer.8.ff.layer_1.bias\", \"model.layer.8.ff.layer_2.weight\", \"model.layer.8.ff.layer_2.bias\", \"model.layer.9.rel_attn.q\", \"model.layer.9.rel_attn.k\", \"model.layer.9.rel_attn.v\", \"model.layer.9.rel_attn.o\", \"model.layer.9.rel_attn.r\", \"model.layer.9.rel_attn.r_r_bias\", \"model.layer.9.rel_attn.r_s_bias\", \"model.layer.9.rel_attn.r_w_bias\", \"model.layer.9.rel_attn.seg_embed\", \"model.layer.9.rel_attn.layer_norm.weight\", \"model.layer.9.rel_attn.layer_norm.bias\", \"model.layer.9.ff.layer_norm.weight\", \"model.layer.9.ff.layer_norm.bias\", \"model.layer.9.ff.layer_1.weight\", \"model.layer.9.ff.layer_1.bias\", \"model.layer.9.ff.layer_2.weight\", \"model.layer.9.ff.layer_2.bias\", \"model.layer.10.rel_attn.q\", \"model.layer.10.rel_attn.k\", \"model.layer.10.rel_attn.v\", \"model.layer.10.rel_attn.o\", \"model.layer.10.rel_attn.r\", \"model.layer.10.rel_attn.r_r_bias\", \"model.layer.10.rel_attn.r_s_bias\", \"model.layer.10.rel_attn.r_w_bias\", \"model.layer.10.rel_attn.seg_embed\", \"model.layer.10.rel_attn.layer_norm.weight\", \"model.layer.10.rel_attn.layer_norm.bias\", \"model.layer.10.ff.layer_norm.weight\", \"model.layer.10.ff.layer_norm.bias\", \"model.layer.10.ff.layer_1.weight\", \"model.layer.10.ff.layer_1.bias\", \"model.layer.10.ff.layer_2.weight\", \"model.layer.10.ff.layer_2.bias\", \"model.layer.11.rel_attn.q\", \"model.layer.11.rel_attn.k\", \"model.layer.11.rel_attn.v\", \"model.layer.11.rel_attn.o\", \"model.layer.11.rel_attn.r\", \"model.layer.11.rel_attn.r_r_bias\", \"model.layer.11.rel_attn.r_s_bias\", \"model.layer.11.rel_attn.r_w_bias\", \"model.layer.11.rel_attn.seg_embed\", \"model.layer.11.rel_attn.layer_norm.weight\", \"model.layer.11.rel_attn.layer_norm.bias\", \"model.layer.11.ff.layer_norm.weight\", \"model.layer.11.ff.layer_norm.bias\", \"model.layer.11.ff.layer_1.weight\", \"model.layer.11.ff.layer_1.bias\", \"model.layer.11.ff.layer_2.weight\", \"model.layer.11.ff.layer_2.bias\", \"classifer.weight\", \"classifer.bias\". "
     ]
    }
   ],
   "source": [
    "test_set=copy.deepcopy(test_dataset)\n",
    "acc, prec_0, rec_0, f_0, prec_1, rec_1, f_1 = test_text_epoch(1,test_set)\n",
    "print('full metric under best accuracy: acc, prec, rec, F1, prec_fake, rec_fake, F1_fake')\n",
    "print(acc, prec_0, rec_0, f_0, prec_1, rec_1, f_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from conf import config\n",
    "# best_text_acc = 0\n",
    "# text_save_file = None\n",
    "# train_set=copy.deepcopy(train_dataset)\n",
    "# test_set=copy.deepcopy(test_dataset)\n",
    "# sys.stdout = Logger(os.path.join(config.save_folder, 'text.txt'))\n",
    "# for epoch_id in range(60):\n",
    "#     print('Epoch:', epoch_id)\n",
    "#     random.shuffle(train_set)\n",
    "#     train_text_epoch(epoch_id, train_set)\n",
    "#     test_text_acc = test_text_epoch(epoch_id,test_set)\n",
    "#     if test_text_acc > best_text_acc:\n",
    "#         best_text_acc = test_text_acc\n",
    "#         state = {\n",
    "#             'epoch': epoch_id,\n",
    "#             'model': model_text.state_dict(),\n",
    "#             'best_acc': best_text_acc,\n",
    "#         }\n",
    "#         text_save_file = os.path.join(config.save_folder, 'text_best.pth')\n",
    "#         print('saving the best model for text!')\n",
    "#         torch.save(state, text_save_file)\n",
    "#         print('\\n')\n",
    "# print('best accuracy for text model :', best_text_acc)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
